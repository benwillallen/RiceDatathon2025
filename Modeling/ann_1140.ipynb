{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# torhc dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from torchviz import make_dot\n",
    "\n",
    "#import summary for models \n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test: pd.DataFrame = pd.read_csv('../Data/train_validate.csv')\n",
    "\n",
    "# # Apply SMOTE to the training set\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Make a pytorch dataset\n",
    "class EEGDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  \n",
    "        self.y = torch.tensor(y, dtype=torch.float32)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train, test = train_test_split(train_test, test_size=0.2, random_state=42)\n",
    "train: pd.DataFrame\n",
    "X_train = train.drop(columns=train.filter(regex='main.*'))\n",
    "y_train = train.filter(regex='main.*')\n",
    "X_validate = test.drop(columns=test.filter(regex='main.*'))\n",
    "y_validate = test.filter(regex='main.*')\n",
    "\n",
    "# Apply pca to the training set\n",
    "pca = PCA(n_components=300)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_validate = pca.transform(X_validate)\n",
    "    \n",
    "train_dataset = EEGDS(X_train, y_train.to_numpy())\n",
    "validate_dataset = EEGDS(X_validate, y_validate.to_numpy())\n",
    "\n",
    "# data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, neurons, dropout=0.10):\n",
    "        super(EEGClassifier, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, neurons[0])\n",
    "        self.layer2 = nn.Linear(neurons[0], neurons[1])\n",
    "        self.layer3 = nn.Linear(neurons[1], neurons[2])\n",
    "        self.layer4 = nn.Linear(neurons[2], neurons[3])\n",
    "        self.layer5 = nn.Linear(neurons[3], neurons[4])\n",
    "        self.output_layer = nn.Linear(neurons[4], output_dim)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(neurons[0])\n",
    "        self.bn2 = nn.BatchNorm1d(neurons[1])\n",
    "        self.bn3 = nn.BatchNorm1d(neurons[2])\n",
    "        self.bn4 = nn.BatchNorm1d(neurons[3])\n",
    "        self.bn5 = nn.BatchNorm1d(neurons[4])\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        init.xavier_uniform_(self.layer1.weight)\n",
    "        init.xavier_uniform_(self.layer2.weight)\n",
    "        init.xavier_uniform_(self.layer3.weight)\n",
    "        init.xavier_uniform_(self.layer4.weight)\n",
    "        init.xavier_uniform_(self.layer5.weight)\n",
    "        init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.gelu(self.bn1(self.layer1(x))) \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.bn2(self.layer2(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.bn3(self.layer3(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.bn4(self.layer4(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.gelu(self.bn5(self.layer5(x)))\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1985, -0.0298, -0.0870, -0.0279,  0.0346,  0.0324, -0.1007]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EEGClassifier(300, 7, [512, 256, 128, 64, 32], 0.1)\n",
    "model.eval()\n",
    "sample_input = torch.randn(1, 300)  \n",
    "\n",
    "output = model(sample_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 512]         154,112\n",
      "       BatchNorm1d-2                  [-1, 512]           1,024\n",
      "              GELU-3                  [-1, 512]               0\n",
      "           Dropout-4                  [-1, 512]               0\n",
      "            Linear-5                  [-1, 256]         131,328\n",
      "       BatchNorm1d-6                  [-1, 256]             512\n",
      "              GELU-7                  [-1, 256]               0\n",
      "           Dropout-8                  [-1, 256]               0\n",
      "            Linear-9                  [-1, 128]          32,896\n",
      "      BatchNorm1d-10                  [-1, 128]             256\n",
      "             GELU-11                  [-1, 128]               0\n",
      "          Dropout-12                  [-1, 128]               0\n",
      "           Linear-13                   [-1, 64]           8,256\n",
      "      BatchNorm1d-14                   [-1, 64]             128\n",
      "             GELU-15                   [-1, 64]               0\n",
      "          Dropout-16                   [-1, 64]               0\n",
      "           Linear-17                   [-1, 32]           2,080\n",
      "      BatchNorm1d-18                   [-1, 32]              64\n",
      "             GELU-19                   [-1, 32]               0\n",
      "           Linear-20                    [-1, 7]             231\n",
      "================================================================\n",
      "Total params: 330,887\n",
      "Trainable params: 330,887\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.03\n",
      "Params size (MB): 1.26\n",
      "Estimated Total Size (MB): 1.29\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(300, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamsterlord/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.0001,\n",
    "                       weight_decay=1e-5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, \n",
    "                              mode='min', \n",
    "                              factor = 0.1, \n",
    "                              patience = 3, \n",
    "                              min_lr = 1e-5, \n",
    "                              verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300])\n",
      "torch.Size([32, 7])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    inputs, labels = data\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.2360\n",
      "Validation Loss: 2.0920, Validation Accuracy: 0.1293\n",
      "\n",
      "Epoch [2/100], Loss: 2.1674\n",
      "Validation Loss: 2.0733, Validation Accuracy: 0.1709\n",
      "\n",
      "Epoch [3/100], Loss: 2.1252\n",
      "Validation Loss: 2.0538, Validation Accuracy: 0.1657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = np.inf\n",
    "patience = 20\n",
    "min_delta = 0.01\n",
    "early_stop_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "final_epoch = 0\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    running_accuracy = 0.\n",
    "\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()  # zero gradients\n",
    "        \n",
    "        outputs = model.forward(inputs)  # input net\n",
    "        \n",
    "        #print(type(outputs))\n",
    "        \n",
    "        labels = labels.argmax(dim=1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "#         outputs = F.softmax(outputs, dim=1)\n",
    "#         top_p, top_class = outputs.topk(k=1, dim=1)\n",
    "\n",
    "#         equals = top_class == labels.view(*top_class.shape)\n",
    "\n",
    "#         running_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "#         _, predicted = torch.max(outputs.data, 1)  \n",
    "#         accuracy = (predicted == labels).float().mean()\n",
    "#         running_accuracy += accuracy.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    #epoch_accuracy = running_accuracy / len(train_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # val\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    correct = total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validate_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.float()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            \n",
    "            accuracy = (predicted == labels).float().mean()\n",
    "            val_accuracy += accuracy.item()\n",
    "\n",
    "    val_loss /= len(validate_loader)\n",
    "    val_accuracy /= len(validate_loader)\n",
    "\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\\n\")\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_accuracy)\n",
    "\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss - min_delta:\n",
    "        best_loss = val_loss\n",
    "        early_stop_counter = 0  \n",
    "        #best_model_state = model.state_dict()  \n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    # if early_stop_counter >= patience:\n",
    "    #     print(f\"Early stopping at epoch {epoch+1}.\")\n",
    "    #     final_epoch = epoch+1\n",
    "    #     #model.load_state_dict(best_model_state) \n",
    "    #     break\n",
    "        \n",
    "    scheduler.step(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
